dat$Article
table(dat$Article)
sum(table(dat$Article)==1)
length(table(dat$Article)==1)
length(table(dat$Article))
dat$Year
dat$Year - 2010
head(dat)
glmer(measure.of.latent.process ~ I(Year - 2010) + (1 | Article), family = binomial, data = dat)
m_lat = glmer(measure.of.latent.process ~ I(Year - 2010) + (1 | Article),
family = binomial, data = dat)
summary(m_lat)
m1_lat = glmer(measure.of.latent.process ~ I(Year - 2010) + (1 | Article),
family = binomial, data = dat)
summary(m1_lat)
m2_lat = glmer(measure.of.latent.process ~ poly(I(Year - 2010), 2) + (1 | Article),
family = binomial, data = dat)
summary(m2_lat)
predict(m1_lat)
?predict
predict(m1_lat, type="response")
predict(m1_lat, newdata=data.frame(Year=2011:2020))
predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA))
predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA), re.form=NA)
plogis(predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA), re.form=NA))
plot(2011:2020, plogis(predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA), re.form=NA)))
plot(2011:2020, (predict(m1_lat, newdata=data.frame(Year=2011:2020,
Article=NA),
re.form=NA, link="response")))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=2011:2020,
Article=NA),
re.form=NA, link="response"))
data.frame(Year=2011:2020, Article=NA)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA),
re.form=NA, link="response"))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA),
re.form=NA, type="response"))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=1:1-, Article=NA),
re.form=NA, type="response"))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=1:10, Article=NA),
re.form=NA, type="response"))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=2011:2020, Article=NA),
re.form=NA, type="response"))
dat.t
with(dat.t, points(Year, Prop_LatentProcess))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=2011:2020),
re.form=NA, type="response"))
with(dat.t, points(Year, Prop_LatentProcess))
m1_lat = glmer(measure.of.latent.process ~ I(Year - 2010) + (1 | Article),
family = binomial, data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year=2011:2020),
re.form=NA, type="response"))
with(dat.t, points(Year, Prop_LatentProcess))
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year=2011:2020),
re.form=NA, type="response"))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year=2011:2020),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
dat.t
m1_lat
dat$Year2 = dat$Year - 2010
m1_lat = glmer(measure.of.latent.process ~ Year2 + (1 | Article),
family = binomial, data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
re.form=NA, type="response"))
with(dat.t, points(Year-2010, Prop_LatentProcess, col="red"))
with(dat.t, points(as.numeric(as.character(Year))-2010, Prop_LatentProcess, col="red"))
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(as.numeric(as.character(Year))-2010, Prop_LatentProcess, col="red"))
dat$Year2
dat$Year2 = dat$Year - 2010
m1_lat = glmer(measure.of.latent.process ~ Year2 + (1 | Article),
family = binomial, data = dat)
summary(m1_lat)
plot(1:10, predict(m1_lat, newdata=data.frame(Year2=1:10),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
m1_lat
dat$measure.of.latent.process
mean(dat$measure.of.latent.process)
m1_lat = glmer(measure.of.latent.process ~ Year2 + (1 | Article),
family = binomial(link = "logit"), data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
re.form=NA, type="response"), ylim=c(0,.5))
glm(measure.of.latent.process ~ Year2, family = binomial(link="logit"), data = dat)
m1_lat = glm(measure.of.latent.process ~ Year2, family = binomial(link="logit"), data = dat)
m1_lat
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
## glmer doesn't work because the majority of studies were single experiment
table(dat$Article) == 1
## glmer doesn't work because the majority of studies were single experiment
sum(table(dat$Article) == 1)
## glmer doesn't work because the majority of studies were single experiment
sum(table(dat$Article) == 1) / length(unique(dat$Article))
m2_lat = glm(measure.of.latent.process ~ ploy(Year2, 2), family = binomial(link="logit"), data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.5))
m2_lat = glm(measure.of.latent.process ~ ploy(Year2, 2), family = binomial(link="logit"), data = dat)
m2_lat = glm(measure.of.latent.process ~ poly(Year2, 2), family = binomial(link="logit"), data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.5))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
# use glm and ignore (slight) clustering
m1_lat = glm(measure.of.latent.process ~ Year2, family = binomial(link="logit"), data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
m2_lat = glm(measure.of.latent.process ~ poly(Year2, 2), family = binomial(link="logit"), data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6)
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
}
```
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
anova(m1_lat, m2_lat)
car:Anova(m1_lat, m2_lat)
car::Anova(m1_lat, m2_lat)
m1_lat
anova(m1_lat, m2_lat, test="Chisq")
##
m1_fit = glm(fit.model ~ Year2, family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m1_fit)
plot(2011:2020, predict(m1_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
dat.t
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel, col="red"))
dat <- read.csv("paper/psych_aging_master.csv")
dat.t <- dat %>%
group_by(Year) %>%
summarise(no_rows = length(Year), Prop_LatentProcess = mean(measure.of.latent.process),
Prop_FitModel = mean(fit.model),
Prop_FitModel_given_LatentProcess = mean(fit.model[measure.of.latent.process==1])
#Prop_FitModel_given_LatentProcess = Prop_FitModel/Prop_LatentProcess
)
dat.t$Year <- factor(dat.t$Year)
fig_a <- ggplot(data=dat.t,
mapping = aes(x = Year, y = Prop_LatentProcess,
group=1)) +
geom_point() + geom_line() +
labs(title="DV = Measure of Latent Processes", tag="a") +
theme_bw() +
theme(panel.grid.minor=element_blank(),
panel.grid.major=element_blank(),
axis.line=element_line(size=0.5, linetype = "solid",
color="black")) +
scale_y_continuous(name="Proportion of Studies", limits=c(0,1.0))
fig_b <- ggplot(data=dat.t,
mapping = aes(x = Year, y = Prop_FitModel_given_LatentProcess,
group=1)) +
geom_point() + geom_line() +
labs(title="Fit a Cognitive Model to Latent Process DV", tag="b") +
theme_bw() +
theme(panel.grid.minor=element_blank(),
panel.grid.major=element_blank(),
axis.line=element_line(size=0.5, linetype = "solid",
color="black")) +
scale_y_continuous(name="Proportion of Studies", limits=c(0,1.0))
comb <- ggarrange(fig_a, fig_b, ncol=1, nrow=2)
comb
dat.t$Prop_FitModel_given_LatentProcess
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel_given_LatentProcess, col="red"))
##
m1_fit = glm(fit.model ~ Year2, family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m1_fit)
plot(2011:2020, predict(m1_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel_given_LatentProcess, col="red"))
m2_fit = glm(fit.model ~ ploy(Year2, 2), family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
m2_fit = glm(fit.model ~ poly(Year2, 2), family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m2_fit)
plot(2011:2020, predict(m2_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
m2_fit = glm(fit.model ~ poly(Year2, 2), family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
table(dat$Article)
dat$Year2 = dat$Year - 2010
m1_lat = glmer(measure.of.latent.process ~ Year2 + (1 | Article),
family = binomial(link = "logit"), data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
m2_lat = glmer(measure.of.latent.process ~ poly(I(Year - 2010), 2) + (1 | Article),
family = binomial, data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year=2011:2020),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
## glmer doesn't work because the majority of studies were single experiment
sum(table(dat$Article) == 1) / length(unique(dat$Article))
# use glm and ignore (slight) clustering
m1_lat = glm(measure.of.latent.process ~ Year2, family = binomial(link="logit"), data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
m2_lat = glm(measure.of.latent.process ~ poly(Year2, 2), family = binomial(link="logit"), data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
anova(m1_lat, m2_lat, test="Chisq")
##
m1_fit = glm(fit.model ~ Year2, family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m1_fit)
plot(2011:2020, predict(m1_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel_given_LatentProcess, col="red"))
m2_fit = glm(fit.model ~ poly(Year2, 2), family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m2_fit)
plot(2011:2020, predict(m2_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel_given_LatentProcess, col="red"))
anova(m2_fit, m2_fit, test="Chisq")
anova(m1_fit, m2_fit, test="Chisq")
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
anova(m1_lat, m2_lat, test="Chisq")
summary(m2_lat)
anova(m1_fit, m2_fit, test="Chisq")
summary(m1_fit)
# data = red; model = black
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
often
# Chunk 1: setup
knitr::opts_chunk$set(echo = F)
#setwd("../code/")
knitr::opts_knit$set(root.dir = normalizePath("../"))
library(rstan)
library(bayesplot)
library(xtable)
SDT <- function(d, k, s = 1){
f = pnorm(-d/2 - k)
h = pnorm((d/2 - k)/(1/s))
return(c(f = f, h = h))
}
ratingModel <- function(d, s, a, b, C, ratingProbs = T, lastPoint = T){
# the parsimonius model from Selker et al
# https://osf.io/v3b76/
unb = -log((1 - 1:C/C)/(1:C/C))
thresh = a*unb + b
f = (diff(pnorm(c(-Inf, thresh),0,1)))
h = (diff(pnorm(c(-Inf, thresh),d,s)))
if (!ratingProbs){
f = cumsum(rev(f))
h = cumsum(rev(h))
}
if(!lastPoint){
f = f[1:(C-1)]
h = h[1:(C-1)]
}
return(cbind(f, h))
}
plotSDT <- function(d = 2, s = 1, a = 1, b = 0, C = 6, title = F, allpars=F){
par(pty='m', mar=c(3,2,.5,.5))
newcol = col2rgb("dodgerblue4")[,1]
newcol = rgb(red = newcol[1], green = newcol[2], blue = newcol[3], alpha = 50, maxColorValue = 255)
oldcol=rgb(1,1,1,.5)
xrange = c(-3, d+3*s)
yrange = c(0, max(dnorm(0), dnorm(d, d, s)))
yscale = ifelse(allpars, 1.8, 1.5)
xseq = seq(from=xrange[1], to=xrange[2], length.out = 1000)
unb = -log((1 - 1:C/C)/(1:C/C))
thresh = a*unb + b
thresh = thresh[1:(C-1)]
plot(NA, ylim = yrange*c(0,yscale), xlim = xrange, ylab="", xlab="", axes=F)
polygon(c(xrange[1], xseq, xrange[2]), c(0, dnorm(xseq), 0), col = newcol , border = NA)
curve(expr = dnorm(x, 0, 1), from=xrange[1], to=xrange[2], n = 10000, add = T)
polygon(c(xrange[1], xseq, xrange[2]), c(0, dnorm(xseq, d, s), 0), col = oldcol , border = NA)
curve(expr = dnorm(x, d, s), from=xrange[1], to=xrange[2], n = 10000, add = T)
for (k in 1:(C-1)){
lines(x = c(thresh[k], thresh[k]), y = yrange*c(0, 1.35), lty=2, col='darkgrey')
text(x = thresh[k], y = yrange[2]*1.4, labels = bquote('c'[.(k)]))
}
label_x = c(xrange[1], thresh, xrange[2])[1:C] + diff(c(xrange[1], thresh, xrange[2]))/2
text(x = label_x, y = yrange[2]*1.2, labels = 1:C, col='red')
if (allpars){
dlaby = yrange[2]*1.6
segments(x0 = 0, x1 = d, y0 = dlaby)
segments(x0 = 0, x1 = 0, y0 = dlaby-yrange[2]*.05, y1 = dlaby+yrange[2]*.05)
segments(x0 = d, x1 = d, y0 = dlaby-yrange[2]*.05, y1 = dlaby+yrange[2]*.05)
text(x = d/2, y = dlaby+yrange[2]*.1, labels = bquote(italic(d)))
slaby = dnorm(d, d, s)/2
segments(x0 = d, x1 = d+s, y0 = slaby)
segments(x0 = d, x1 = d, y0 = slaby-yrange[2]*.05, y1 = slaby+yrange[2]*.05)
segments(x0 = d+s, x1 = d+s, y0 = slaby-yrange[2]*.05, y1 = slaby+yrange[2]*.05)
text(x = d+s/2, y = slaby+yrange[2]*.1, labels = bquote(italic(s)))
}
axis(1)#; box()
if (title){
mtext(text = paste0("d = ", d, ", s = ", s, ", a = ", a, ", b = ", b))
}
}
#plotSDT(d = ps[1], s = ps[2], a = ps[3], b = ps[4], C = ps[5], allpars = T)
# Chunk 2: pa
library(ggplot2)
library(ggpubr)
library(dplyr)
dat <- read.csv("paper/psych_aging_master.csv")
dat.t <- dat %>%
group_by(Year) %>%
summarise(no_rows = length(Year), Prop_LatentProcess = mean(measure.of.latent.process),
Prop_FitModel = mean(fit.model),
Prop_FitModel_given_LatentProcess = mean(fit.model[measure.of.latent.process==1])
#Prop_FitModel_given_LatentProcess = Prop_FitModel/Prop_LatentProcess
)
dat.t$Year <- factor(dat.t$Year)
#write.csv(dat.t, "psych_aging_tibble.csv")
fig_a <- ggplot(data=dat.t,
mapping = aes(x = Year, y = Prop_LatentProcess,
group=1)) +
geom_point() + geom_line() +
labs(title="DV = Measure of Latent Processes", tag="a") +
theme_bw() +
theme(panel.grid.minor=element_blank(),
panel.grid.major=element_blank(),
axis.line=element_line(size=0.5, linetype = "solid",
color="black")) +
scale_y_continuous(name="Proportion of Studies", limits=c(0,1.0))
fig_b <- ggplot(data=dat.t,
mapping = aes(x = Year, y = Prop_FitModel_given_LatentProcess,
group=1)) +
geom_point() + geom_line() +
labs(title="Fit a Cognitive Model to Latent Process DV", tag="b") +
theme_bw() +
theme(panel.grid.minor=element_blank(),
panel.grid.major=element_blank(),
axis.line=element_line(size=0.5, linetype = "solid",
color="black")) +
scale_y_continuous(name="Proportion of Studies", limits=c(0,1.0))
comb <- ggarrange(fig_a, fig_b, ncol=1, nrow=2)
comb
## attempt an analysis
if (F){ # so this doesn't appear in the article
library(lme4)
table(dat$Article)
dat$Year2 = dat$Year - 2010
m1_lat = glmer(measure.of.latent.process ~ Year2 + (1 | Article),
family = binomial(link = "logit"), data = dat)
summary(m1_lat)
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
# quadratic
m2_lat = glmer(measure.of.latent.process ~ poly(I(Year - 2010), 2) + (1 | Article),
family = binomial, data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year=2011:2020),
re.form=NA, type="response"), ylim=c(0,.5))
with(dat.t, points(Year, Prop_LatentProcess, col="red"))
## glmer doesn't work (see predictions) because the
## majority of studies were single experiment
sum(table(dat$Article) == 1) / length(unique(dat$Article))
###
# instead, use glm and ignore (slight) clustering
m1_lat = glm(measure.of.latent.process ~ Year2,
family = binomial(link="logit"), data = dat)
summary(m1_lat)
# data = red; model = black
plot(2011:2020, predict(m1_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
m2_lat = glm(measure.of.latent.process ~ poly(Year2, 2), family = binomial(link="logit"), data = dat)
summary(m2_lat)
plot(2011:2020, predict(m2_lat, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_LatentProcess, col="red"))
anova(m1_lat, m2_lat, test="Chisq")
# summary: it looks more like the quadratic model (even if p=.06...)
# both models suggest significant increase in latent measure over time
##
m1_fit = glm(fit.model ~ Year2, family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m1_fit)
plot(2011:2020, predict(m1_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel_given_LatentProcess, col="red"))
m2_fit = glm(fit.model ~ poly(Year2, 2), family = binomial(link="logit"),
data = subset(dat, measure.of.latent.process==1))
summary(m2_fit)
plot(2011:2020, predict(m2_fit, newdata=data.frame(Year2=1:10),
type="response"), ylim=c(0,.6))
with(dat.t, points(as.numeric(as.character(Year)), Prop_FitModel_given_LatentProcess, col="red"))
anova(m1_fit, m2_fit, test="Chisq")
# summary: increase in fitting models given measuring latent process is n.s.
}
# Chunk 3
# library(papaja)
# library(knitr)
# library(kableExtra)
other_tab = read.csv("paper/other-models-table.csv")
#kable(other_tab, format = "markdown", booktabs=T)
strCaption <- paste0("Selected models that were also used in papers identified by our review of Psychology and Aging between 2011 and 2020. The references and related/competing models are not exhaustive.")
cat("\\begin{landscape}")
print(xtable(other_tab, caption = strCaption, label = 'tab:oth',
align = c("p{0.0\\textwidth}",
"p{0.2\\textwidth}",
"p{1\\textwidth}",
"p{0.30\\textwidth}",
"p{0.50\\textwidth}")), caption.placement = 'top', hline.after = NULL, include.colnames = F, include.rownames = F, sanitize.text.function = force, floating=TRUE, table.placement = 't', comment=F, scalebox = .65, add.to.row = list(pos = list(-1, nrow(other_tab)),       command = c(paste0("\\toprule \n", "Model & Brief description & Selected references & Selected related/competing models\\\\\n","\\midrule \n"), paste0("\\bottomrule \\\\\ "))))
cat("\\end{landscape}")
# Chunk 4: sdt
ps = c(2, 1.2, 1, .5, 6)
# layout(mat = cbind(c(1,1), c(1,1), c(2,3)))
layout(mat = rbind(c(1,1), c(2,3)), heights = c(1.5, 1))
# model plot
plotSDT(d = ps[1], s = ps[2], a = ps[3], b = ps[4], C = ps[5], allpars = T)
# proportions
noi_pch = 1; sig_pch = 2
noi_col = "black"; sig_col = "darkgrey"
par(pty='m', mar=c(4,4,.5,.5))
x=ratingModel(d = ps[1], s = ps[2], a = ps[3], b = ps[4], C = ps[5])
plot(x[,1], pch=noi_pch, col=noi_col, type="b",
ylim=c(0,max(x)*1.2), xlab = "Rating", ylab="p(rating | trial type)") # noise
points(x[,2], pch=sig_pch, col=sig_col, type="b") # signal
text(x = 2, y = x[2,1], labels = "noise", adj = c(1.1,-.5), col = noi_col)
text(x = 5, y = x[5,2], labels = "signal", adj = c(1.1,-.5), col = sig_col)
# roc
par(pty='s', mar=c(2,2,.5,.5))
plot(ratingModel(d = ps[1], s = ps[2], a = ps[3], b = ps[4], C = 1000, ratingProbs = F),
ylim=c(0,1), xlim=c(0,1), type='l', axes=F, xlab="", ylab="")
#xlab='false-alarm rate', ylab='hit rate')
box()
axis(1, at=c(0,1))
axis(2, at=c(0,1))
a = ratingModel(d = ps[1], s = ps[2], a = ps[3], b = ps[4], C = ps[5], ratingProbs = F, lastPoint=F)
points(a, ylim=c(0,1), xlim=c(0,1), pch=21, bg='grey')
mtext("false-alarms", 1, line = 1)
mtext("hits", 2, line = 1)
text(t(t(a)+c(.08,-.08)), labels =
as.expression(unlist(lapply(X = 5:1, function(x) bquote(c[.(x)])))))
# Chunk 5
rdat = read.csv("data/example-data.csv")
head(rdat)
# Chunk 6
syn_tab = read.csv("paper/syntax-table.csv")
# column_spec(knitr::kable(syn_tab, format = "latex", booktabs=T), c(1,2), width = c("5cm", "10cm"))
strCaption <- paste0("Selected parts of the Stan syntax used in the present example with a brief explanation")
print(xtable(syn_tab, caption = strCaption, label = 'tab:syn',
align = c("p{0.0\\textwidth}",
"p{0.40\\textwidth}",
"p{0.80\\textwidth}")), caption.placement = 'top', hline.after = NULL, include.colnames = F, include.rownames = F, sanitize.text.function = function(x) x, floating=TRUE, table.placement = '!h', comment=F, scalebox = .7, add.to.row = list(pos = list(-1, nrow(syn_tab)),       command = c(paste0("\\toprule \n", "Syntax &  Explanation\\\\\n","\\midrule \n"), paste0("\\bottomrule \\\\\ "))))
# Chunk 7
data_list = list(
N = nrow(rdat), # number of observations (trials)
y = rdat$rating, # response
J = length(unique(rdat$id)), # number of participants
id = rdat$id, # participant ids
X = model.matrix(~ 1 + group, data = rdat), # predictors
sig_trial = rdat$signal, # was this a signal trial? (1 = yes, 0 = no)
K = 6 # number of rating categories
)
# Chunk 9
# read saved model files
SDT_m1_fit = readRDS("models/SDT_m1_fit.rds")
yrep <- extract(SDT_m1_fit, pars = "y_rep")[[1]]
y <- rdat$rating
yrep = yrep[sample(1:nrow(yrep), size = 500),]
age_d = extract(SDT_m1_fit, pars="B_d[2]")[[1]]
# Chunk 11
# plot a histogram
hist(age_d, breaks=20, xlab="", main=bquote(Beta[1]^'(d)'), probability = T)
# Chunk 12
# posterior mean
mean(age_d)
# median and 95% credible interval
quantile(age_d, probs = c(0.025, .5, 0.975))
# 95% highest density interval
library(HDInterval)
hdi(age_d, credMass = .95)
# Chunk 13
# extract the group-level effects for d
B_d = extract(SDT_m1_fit, pars="B_d")[[1]]
# column 1 of B_d is the intercept and column 2 is the age difference
# the younger group is the intercept as they were coded zero
d_young = exp( B_d[,1] ) # transform back to natural scale
d_old = exp( B_d[,1] + B_d[,2] )
hdi(d_young)
hdi(d_old)
hdi(d_young - d_old) # HDI for the group difference
# Chunk 14
data_list$X = model.matrix(~ 1 + group + cond + group:cond, data = rdat)
data_list$Z = model.matrix(~ 1 + cond, data = rdat)
dat.t
